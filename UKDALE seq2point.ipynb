{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_appliance = {\n",
    "    'kettle': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 2000,\n",
    "        'max_on_power': 3998,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [10, 8],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'microwave': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 200,\n",
    "        'max_on_power': 3969,\n",
    "        'mean': 500,\n",
    "        'std': 800,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [13, 15],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'fridge': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 50,\n",
    "        'max_on_power': 3323,\n",
    "        'mean': 200,\n",
    "        'std': 400,\n",
    "        's2s_length': 512,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [12, 14],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    \n",
    "   # 'television': { \n",
    "   #     'windowlength': 599,  # You may keep or adjust this\n",
    "   #     'on_power_threshold': 10,\n",
    "   #     'max_on_power': ?,     # To be calculated from meter data\n",
    "   #     'mean': ?,             # Requires usage data\n",
    "   #     'std': ?,              # Requires usage data\n",
    "   #     's2s_length': ?,       # Typical sequence length for s2s models\n",
    "   #     'houses': [1],\n",
    "   #     'channels': [7],\n",
    "   #     'train_build': [1],\n",
    "   #     'test_build': []\n",
    "   # }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "DATA_DIRECTORY = 'dataset/ukdale/'\n",
    "SAVE_PATH = 'ukdale_seq2point/microwave/'\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n",
    "APPLIANCE_NAME = 'microwave'\n",
    "\n",
    "\n",
    "\n",
    "mains_data = {\n",
    "    \"mean\": 522,\n",
    "    \"std\":  814        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae31f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_dataframe(directory, building, channel, col_names=['time', 'data'], nrows=None):\n",
    "    df = pd.read_table(directory + 'house_' + str(building) + '/' + 'channel_' +\n",
    "                       str(channel) + '.dat',\n",
    "                       sep=\"\\s+\",\n",
    "                       nrows=nrows,\n",
    "                       usecols=[0, 1],\n",
    "                       names=col_names,\n",
    "                       dtype={'time': str},\n",
    "                       )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#training_directory=\"ukdale_seq2point/kettle/kettle_training_.csv\"\n",
    "#validation_directory=\"ukdale_seq2point/kettle/kettle_validation_.csv\"\n",
    "save_model_dir=\"saved_models/ukdale_models/\"\n",
    "\n",
    "\n",
    "\n",
    "class get_arguments:\n",
    "    def __init__(self):\n",
    "        self.__data_dir = DATA_DIRECTORY\n",
    "        self.appliance_name = APPLIANCE_NAME\n",
    "        self.aggregate_mean = 522\n",
    "        self.aggregate_std = 814\n",
    "        self.save_path = SAVE_PATH\n",
    "        self.batch_size = 1000\n",
    "        self.crop = 10000\n",
    "        self.prunning_algorithm = \"threshold\"\n",
    "        self.network_type = \"seq2point\"\n",
    "        self.epochs = 20\n",
    "        self.input_window_length = 599\n",
    "        self.validation_frequency = 1\n",
    "        #self.training_directory = training_directory\n",
    "        #self.validation_directory = validation_directory\n",
    "\n",
    "args = get_arguments()\n",
    "appliance_name = args.appliance_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ebc6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13800\\935694265.py:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if training_building_percent is not 0:\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13800\\935694265.py:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if training_building_percent is not 0:\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13800\\935694265.py:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if training_building_percent is not 0:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'get_arguments' object has no attribute '__data_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 111\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal elapsed time: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m min.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m))\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m'\u001b[39m, appliance_name])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m params_appliance[appliance_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhouses\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_dir\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhouse_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(h) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m           \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     15\u001b[0m           \u001b[38;5;28mstr\u001b[39m(params_appliance[appliance_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m][params_appliance[appliance_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhouses\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(h)]) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     16\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m     mains_df \u001b[38;5;241m=\u001b[39m load_dataframe(args\u001b[38;5;241m.\u001b[39m__data_dir, h, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     app_df \u001b[38;5;241m=\u001b[39m load_dataframe(args\u001b[38;5;241m.\u001b[39m__data_dir,\n\u001b[0;32m     20\u001b[0m                             h,\n\u001b[0;32m     21\u001b[0m                             params_appliance[appliance_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m][params_appliance[appliance_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhouses\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(h)],\n\u001b[0;32m     22\u001b[0m                             col_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, appliance_name]\n\u001b[0;32m     23\u001b[0m                             )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'get_arguments' object has no attribute '__data_dir'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    sample_seconds = 8\n",
    "    training_building_percent = 95\n",
    "    validation_percent = 13\n",
    "    nrows = None\n",
    "    debug = False\n",
    "\n",
    "    train = pd.DataFrame(columns=['aggregate', appliance_name])\n",
    "\n",
    "    for h in params_appliance[appliance_name]['houses']:\n",
    "        print('    ' + args.__data_dir + 'house_' + str(h) + '/'\n",
    "              + 'channel_' +\n",
    "              str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "              '.dat')\n",
    "\n",
    "        mains_df = load_dataframe(args.__data_dir, h, 1)\n",
    "        app_df = load_dataframe(args.__data_dir,\n",
    "                                h,\n",
    "                                params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "                                col_names=['time', appliance_name]\n",
    "                                )\n",
    "\n",
    "        mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        mains_df.columns = ['aggregate']\n",
    "        #resample = mains_df.resample(str(sample_seconds) + 'S').mean()\n",
    "        mains_df.reset_index(inplace=True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"    mains_df:\")\n",
    "            print(mains_df.head())\n",
    "            plt.plot(mains_df['time'], mains_df['aggregate'])\n",
    "            plt.show()\n",
    "\n",
    "        # Appliance\n",
    "        app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
    "\n",
    "        if debug:\n",
    "            print(\"app_df:\")\n",
    "            print(app_df.head())\n",
    "            plt.plot(app_df['time'], app_df[appliance_name])\n",
    "            plt.show()\n",
    "\n",
    "        # the timestamps of mains and appliance are not the same, we need to align them\n",
    "        # 1. join the aggragte and appliance dataframes;\n",
    "        # 2. interpolate the missing values;\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        app_df.set_index('time', inplace=True)\n",
    "\n",
    "        df_align = mains_df.join(app_df, how='outer'). \\\n",
    "            resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
    "        df_align = df_align.dropna()\n",
    "\n",
    "        df_align.reset_index(inplace=True)\n",
    "\n",
    "        del mains_df, app_df, df_align['time']\n",
    "\n",
    "        if debug:\n",
    "            # plot the dtaset\n",
    "            print(\"df_align:\")\n",
    "            print(df_align.head())\n",
    "            plt.plot(df_align['aggregate'].values)\n",
    "            plt.plot(df_align[appliance_name].values)\n",
    "            plt.show()\n",
    "\n",
    "        # Normilization ----------------------------------------------------------------------------------------------\n",
    "        mean = params_appliance[appliance_name]['mean']\n",
    "        std = params_appliance[appliance_name]['std']\n",
    "\n",
    "        df_align['aggregate'] = (df_align['aggregate'] - args.aggregate_mean) / args.aggregate_std\n",
    "        df_align[appliance_name] = (df_align[appliance_name] - mean) / std\n",
    "\n",
    "        if h == params_appliance[appliance_name]['test_build']:\n",
    "            # Test CSV\n",
    "            df_align.to_csv(args.save_path + appliance_name + '_test_.csv', mode='a', index=False, header=False)\n",
    "            print(\"    Size of test set is {:.4f} M rows.\".format(len(df_align) / 10 ** 6))\n",
    "            continue\n",
    "\n",
    "        \n",
    "        train = pd.concat([train, df_align], ignore_index=True)\n",
    "        del df_align\n",
    "\n",
    "    # Crop dataset\n",
    "    if training_building_percent is not 0:\n",
    "        train.drop(train.index[-int((len(train)/100)*training_building_percent):], inplace=True)\n",
    "\n",
    "\n",
    "    # Validation CSV\n",
    "    val_len = int((len(train)/100)*validation_percent)\n",
    "    val = train.tail(val_len)\n",
    "    val.reset_index(drop=True, inplace=True)\n",
    "    train.drop(train.index[-val_len:], inplace=True)\n",
    "    # Validation CSV\n",
    "    val.to_csv(args.save_path + appliance_name + '_validation_' + '.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # Training CSV\n",
    "    train.to_csv(args.save_path + appliance_name + '_training_.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    print(\"    Size of total training set is {:.4f} M rows.\".format(len(train) / 10 ** 6))\n",
    "    print(\"    Size of total validation set is {:.4f} M rows.\".format(len(val) / 10 ** 6))\n",
    "    del train, val\n",
    "\n",
    "\n",
    "    print(\"\\nPlease find files in: \" + args.save_path)\n",
    "    print(\"Total elapsed time: {:.2f} min.\".format((time.time() - start_time) / 60))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604dc0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting creating testset...\n",
      "dataset/ukdale/house_1/channel_10.dat\n",
      "         time  kettle\n",
      "0  1352500098       1\n",
      "1  1352500104       1\n",
      "2  1352500110       1\n",
      "3  1352500116       1\n",
      "4  1352500122       1\n",
      "         time  kettle\n",
      "0  1352500095     599\n",
      "1  1352500101     582\n",
      "2  1352500107     600\n",
      "3  1352500113     586\n",
      "4  1352500120     596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:57: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:58: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  kettle\n",
      "0 1970-01-16 15:41:40.095     599\n",
      "1 1970-01-16 15:41:40.101     582\n",
      "2 1970-01-16 15:41:40.107     600\n",
      "3 1970-01-16 15:41:40.113     586\n",
      "4 1970-01-16 15:41:40.120     596\n",
      "                     time  kettle\n",
      "0 1970-01-16 15:41:40.098       1\n",
      "1 1970-01-16 15:41:40.104       1\n",
      "2 1970-01-16 15:41:40.110       1\n",
      "3 1970-01-16 15:41:40.116       1\n",
      "4 1970-01-16 15:41:40.122       1\n",
      "   aggregate  kettle\n",
      "0        599       1\n",
      "1        582       1\n",
      "2        600       1\n",
      "3        586       1\n",
      "4        596       1\n",
      "                     aggregate  kettle\n",
      "1970-01-01 00:00:00      590.5     1.0\n",
      "1970-01-01 00:00:08      600.0     1.0\n",
      "1970-01-01 00:00:16      586.0     1.0\n",
      "1970-01-01 00:00:24      588.5     1.0\n",
      "1970-01-01 00:00:32      597.0     1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:72: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:74: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 1).\n",
      "dataset/ukdale/house_2/channel_8.dat\n",
      "         time  kettle\n",
      "0  1361116822       0\n",
      "1  1361116825       0\n",
      "2  1361116831       0\n",
      "3  1361116837       0\n",
      "4  1361116843       0\n",
      "         time  kettle\n",
      "0  1361117854     340\n",
      "1  1361117860     341\n",
      "2  1361117866     347\n",
      "3  1361117872     350\n",
      "4  1361117878     342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:57: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:58: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  kettle\n",
      "0 1970-01-16 18:05:17.854     340\n",
      "1 1970-01-16 18:05:17.860     341\n",
      "2 1970-01-16 18:05:17.866     347\n",
      "3 1970-01-16 18:05:17.872     350\n",
      "4 1970-01-16 18:05:17.878     342\n",
      "                     time  kettle\n",
      "0 1970-01-16 18:05:16.822       0\n",
      "1 1970-01-16 18:05:16.825       0\n",
      "2 1970-01-16 18:05:16.831       0\n",
      "3 1970-01-16 18:05:16.837       0\n",
      "4 1970-01-16 18:05:16.843       0\n",
      "   aggregate  kettle\n",
      "0        340       0\n",
      "1        341       0\n",
      "2        347       0\n",
      "3        350       0\n",
      "4        342       0\n",
      "                     aggregate  kettle\n",
      "1970-01-01 00:00:00      340.5     0.0\n",
      "1970-01-01 00:00:08      347.0     0.0\n",
      "1970-01-01 00:00:16      350.0     0.0\n",
      "1970-01-01 00:00:24      341.5     0.0\n",
      "1970-01-01 00:00:32      343.0     0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:72: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12424\\500205128.py:74: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 2).\n",
      "\n",
      "Normalization parameters: \n",
      "Mean and standard deviation values USED for AGGREGATE are:\n",
      "    Mean = 814, STD = 522\n",
      "Mean and standard deviation values USED for kettle are:\n",
      "    Mean = 700, STD = 1000\n",
      "\n",
      "Please find files in: ukdale_seq2point/kettle/\n",
      "\n",
      "Total elapsed time: 0 min\n"
     ]
    }
   ],
   "source": [
    "nrows = 10**5\n",
    "path = DATA_DIRECTORY \n",
    "save_path = SAVE_PATH\n",
    "aggregate_std = AGG_MEAN \n",
    "aggregate_mean = AGG_STD\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def load(path, building, appliance, channel, nrows=None):\n",
    "    # load csv\n",
    "    file_name = path + 'house_' + str(building) + '/' + 'channel_' + str(channel) + '.dat'\n",
    "    single_csv = pd.read_csv(file_name,\n",
    "                             sep=' ',\n",
    "                             #header=0,\n",
    "                             names=['time', appliance],\n",
    "                             dtype={'time': str, \"appliance\": int},\n",
    "                             #parse_dates=['time'],\n",
    "                             #date_parser=pd.to_datetime,\n",
    "                             nrows=nrows,\n",
    "                             usecols=[0, 1],\n",
    "                             engine='python'\n",
    "                             )\n",
    "    return single_csv\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting creating testset...\")\n",
    "\n",
    "for h in params_appliance[appliance_name]['houses']:\n",
    "\n",
    "    print(path + 'house_' + str(h) + '/'\n",
    "          + 'channel_' +\n",
    "          str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "          '.dat')\n",
    "\n",
    "    agg_df = load(path,\n",
    "                  h,\n",
    "                  appliance_name,\n",
    "                  1,\n",
    "                  nrows=nrows,\n",
    "                  )\n",
    "\n",
    "    df = load(path,\n",
    "              h,\n",
    "              appliance_name,\n",
    "              params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "              nrows=nrows,\n",
    "              )\n",
    "\n",
    "    #for i in range(100):\n",
    "    #    print(int(df['time'][i]) - int(agg_df['time'][i]))\n",
    "\n",
    "    # Time conversion\n",
    "    print(df.head())\n",
    "    print(agg_df.head())\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n",
    "    print(agg_df.head())\n",
    "    print(df.head())\n",
    "\n",
    "    df['aggregate'] = agg_df[appliance_name]\n",
    "    cols = df.columns.tolist()\n",
    "    del cols[0]\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    # Re-sampling\n",
    "    ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
    "    df.set_index(ind, inplace=True, drop=True)\n",
    "    resample = df.resample('8S')\n",
    "    df = resample.mean()\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # Normalization\n",
    "    df['aggregate'] = (df['aggregate'] - aggregate_mean) / aggregate_std\n",
    "    df[appliance_name] = \\\n",
    "        (df[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name]['std']\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(save_path + appliance_name + '_test_' + 'uk-dale_' + 'H' + str(h) + '.csv', index=False)\n",
    "\n",
    "    print(\"Size of test set is {:.3f} M rows (House {:d}).\"\n",
    "          .format(df.shape[0] / 10 ** 6, h))\n",
    "\n",
    "    del df\n",
    "\n",
    "\n",
    "print(\"\\nNormalization parameters: \")\n",
    "print(\"Mean and standard deviation values USED for AGGREGATE are:\")\n",
    "print(\"    Mean = {:d}, STD = {:d}\".format(aggregate_mean, aggregate_std))\n",
    "\n",
    "print('Mean and standard deviation values USED for ' + appliance_name + ' are:')\n",
    "print(\"    Mean = {:d}, STD = {:d}\"\n",
    "      .format(params_appliance[appliance_name]['mean'], params_appliance[appliance_name]['std']))\n",
    "\n",
    "print(\"\\nPlease find files in: \" + save_path)\n",
    "tot = int(int(time.time() - start_time) / 60)\n",
    "print(\"\\nTotal elapsed time: \" + str(tot) + ' min')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b9788",
   "metadata": {},
   "source": [
    "Data Feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5447e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# batch_size: the number of rows fed into the network at once.\n",
    "# crop: the number of rows in the data set to be used in total.\n",
    "# chunk_size: the number of lines to read from the file at once.\n",
    "\n",
    "class TrainSlidingWindowGenerator():\n",
    "\n",
    "    \"\"\"Yields features and targets for training a ConvNet.\n",
    "\n",
    "    Parameters:\n",
    "    __file_name (string): The path where the training dataset is located.\n",
    "    __batch_size (int): The size of each batch from the dataset to be processed.\n",
    "    __chunk_size (int): The size of each chunk of data to be processed.\n",
    "    __shuffle (bool): Whether the dataset should be shuffled before being returned.\n",
    "    __offset (int):\n",
    "    __crop (int): The number of rows of the dataset to return.\n",
    "    __skip_rows (int): The number of rows of a dataset to skip before reading data.\n",
    "    __ram_threshold (int): The maximum amount of RAM to utilise at a time.\n",
    "    total_size (int): The number of rows read from the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                file_name, \n",
    "                chunk_size, \n",
    "                shuffle, \n",
    "                offset, \n",
    "                batch_size=1000, \n",
    "                crop=100000, \n",
    "                skip_rows=0, \n",
    "                ram_threshold=5 * 10 ** 5):\n",
    "        self.__file_name = file_name\n",
    "        self.__batch_size = batch_size\n",
    "        self.__chunk_size = 10 ** 8\n",
    "        self.__shuffle = shuffle\n",
    "        self.__offset = offset\n",
    "        self.__crop = crop\n",
    "        self.__skip_rows = skip_rows\n",
    "        self.__ram_threshold = ram_threshold\n",
    "        self.__total_size = 0\n",
    "        self.__total_num_samples = crop\n",
    "\n",
    "    @property\n",
    "    def total_num_samples(self):\n",
    "        return self.__total_num_samples\n",
    "    \n",
    "    @total_num_samples.setter\n",
    "    def total_num_samples(self, value):\n",
    "        self.__total_num_samples = value\n",
    "\n",
    "    def check_if_chunking(self):\n",
    "\n",
    "        \"\"\"Count the number of rows in the dataset and determine whether this is larger than the chunking \n",
    "        threshold or not. \"\"\"\n",
    "\n",
    "        # Loads the file and counts the number of rows it contains.\n",
    "        print(\"Importing training file...\")\n",
    "        chunks = pd.read_csv(self.__file_name, \n",
    "                            header=0, \n",
    "                            nrows=self.__crop, \n",
    "                            skiprows=self.__skip_rows)\n",
    "        print(\"Counting number of rows...\")\n",
    "        self.__total_size = len(chunks)\n",
    "        del chunks\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"The dataset contains \", self.__total_size, \" rows\")\n",
    "\n",
    "        # Display a warning if there are too many rows to fit in the designated amount RAM.\n",
    "        if (self.total_size > self.__ram_threshold):\n",
    "            print(\"There is too much data to load into memory, so it will be loaded in chunks. Please note that this may result in decreased training times.\")\n",
    "    \n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\"Yields pairs of features and targets that will be used directly by a neural network for training.\n",
    "\n",
    "        Yields:\n",
    "        input_data (numpy.array): A 1D array of size batch_size containing features of a single input. \n",
    "        output_data (numpy.array): A 1D array of size batch_size containing the target values corresponding to \n",
    "        each feature set.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__total_size == 0:\n",
    "            self.check_if_chunking()\n",
    "\n",
    "        # If the data can be loaded in one go, don't skip any rows.\n",
    "        if (self.__total_size <= self.__ram_threshold):\n",
    "\n",
    "            # Returns an array of the content from the CSV file.\n",
    "            data_array = np.array(pd.read_csv(self.__file_name, nrows=self.__crop, skiprows=self.__skip_rows, header=0))\n",
    "            inputs = data_array[:, 0]\n",
    "            outputs = data_array[:, 1]\n",
    "\n",
    "            maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "            self.total_num_samples = maximum_batch_size\n",
    "            if self.__batch_size < 0:\n",
    "                self.__batch_size = maximum_batch_size\n",
    "\n",
    "            indicies = np.arange(maximum_batch_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "        # Skip rows where needed to allow data to be loaded properly when there is not enough memory.\n",
    "        if (self.__total_size >= self.__ram_threshold):\n",
    "            number_of_chunks = np.arange(self.__total_size / self.__chunk_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(number_of_chunks)\n",
    "\n",
    "            # Yield the data in sections.\n",
    "            for index in number_of_chunks:\n",
    "                data_array = np.array(pd.read_csv(self.__file_name, skiprows=int(index) * self.__chunk_size, header=0, nrows=self.__crop))                   \n",
    "                inputs = data_array[:, 0]\n",
    "                outputs = data_array[:, 1]\n",
    "\n",
    "                maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "                self.__total_num_samples = maximum_batch_size\n",
    "                if self.__batch_size < 0:\n",
    "                    self.__batch_size = maximum_batch_size\n",
    "\n",
    "                indicies = np.arange(maximum_batch_size)\n",
    "                if self.__shuffle:\n",
    "                    np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "class TestSlidingWindowGenerator(object):\n",
    "\n",
    "    \"\"\"Yields features and targets for testing and validating a ConvNet.\n",
    "\n",
    "    Parameters:\n",
    "    __number_of_windows (int): The number of sliding windows to produce.\n",
    "    __offset (int): The offset of the infered value from the sliding window.\n",
    "    __inputs (numpy.ndarray): The available testing / validation features.\n",
    "    __targets (numpy.ndarray): The target values corresponding to __inputs.\n",
    "    __total_size (int): The total number of inputs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_windows, inputs, targets, offset):\n",
    "        self.__number_of_windows = number_of_windows\n",
    "        self.__offset = offset\n",
    "        self.__inputs = inputs\n",
    "        self.__targets = targets\n",
    "        self.total_size = len(inputs)\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\"Yields features and targets for testing and validating a ConvNet.\n",
    "\n",
    "        Yields:\n",
    "        input_data (numpy.array): An array of features to test / validate the network with.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.__inputs = self.__inputs.flatten()\n",
    "        max_number_of_windows = self.__inputs.size - 2 * self.__offset\n",
    "\n",
    "        if self.__number_of_windows < 0:\n",
    "            self.__number_of_windows = max_number_of_windows\n",
    "\n",
    "        indicies = np.arange(max_number_of_windows, dtype=int)\n",
    "        for start_index in range(0, max_number_of_windows, self.__number_of_windows):\n",
    "            splice = indicies[start_index : start_index + self.__number_of_windows]\n",
    "            input_data = np.array([self.__inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "            target_data = self.__targets[splice + self.__offset].reshape(-1, 1)\n",
    "            yield input_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb862de",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ac4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "\n",
    "def create_model(input_window_length):\n",
    "\n",
    "    \"\"\"Specifies the structure of a seq2point model using Keras' functional API.\n",
    "\n",
    "    Returns:\n",
    "    model (tensorflow.keras.Model): The uncompiled seq2point model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def save_model(model, network_type, algorithm, appliance, save_model_dir):\n",
    "\n",
    "    \"\"\" Saves a model to a specified location. Models are named using a combination of their \n",
    "    target appliance, architecture, and pruning algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The Keras model to save.\n",
    "    network_type (string): The architecture of the model ('', 'reduced', 'dropout', or 'reduced_dropout').\n",
    "    algorithm (string): The pruning algorithm applied to the model.\n",
    "    appliance (string): The appliance the model was trained with.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #model_path = \"saved_models/\" + appliance + \"_\" + algorithm + \"_\" + network_type + \"_model.h5\"\n",
    "    model_path = save_model_dir\n",
    "\n",
    "    if not os.path.exists (model_path):\n",
    "        open((model_path), 'a').close()\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "def load_model(model, network_type, algorithm, appliance, saved_model_dir):\n",
    "\n",
    "    \"\"\" Loads a model from a specified location.\n",
    "\n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The Keas model to which the loaded weights will be applied to.\n",
    "    network_type (string): The architecture of the model ('', 'reduced', 'dropout', or 'reduced_dropout').\n",
    "    algorithm (string): The pruning algorithm applied to the model.\n",
    "    appliance (string): The appliance the model was trained with.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #model_name = \"saved_models/\" + appliance + \"_\" + algorithm + \"_\" + network_type + \"_model.h5\"\n",
    "    model_name = saved_model_dir\n",
    "    print(\"PATH NAME: \", model_name)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    num_of_weights = model.count_params()\n",
    "    print(\"Loaded model with \", str(num_of_weights), \" weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac96e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(string):\n",
    "    return string.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6d7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "class Trainer():\n",
    "\n",
    "    \"\"\" Used to train a seq2point model with or without pruning applied Supports \n",
    "    various alternative architectures. \n",
    "    \n",
    "    Parameters:\n",
    "    __appliance (string): The target appliance.\n",
    "    __network_type (string): The architecture of the model.\n",
    "    __batch_size (int): The number of rows per testing batch.\n",
    "    __window_size (int): The size of eaech sliding window\n",
    "    __window_offset (int): The offset of the inferred value from the sliding window.\n",
    "    __max_chunk_size (int): The largest possible number of row per chunk.\n",
    "    __validation_frequency (int): The number of epochs between model validation.\n",
    "    __training_directory (string): The directory of the model's training file.\n",
    "    __validation_directory (string): The directory of the model's validation file.\n",
    "    __training_chunker (TrainSlidingWindowGenerator): A sliding window provider \n",
    "    that returns feature / target pairs. For training use only.\n",
    "    __validation_chunker (TrainSlidingWindowGenerator): A sliding window provider \n",
    "    that returns feature / target pairs. For validation use only.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, appliance, batch_size, crop, network_type, \n",
    "                 training_directory, validation_directory, save_model_dir,\n",
    "                 epochs=10, input_window_length=599, validation_frequency = 1,\n",
    "                 patience=3, min_delta=1e-6, verbose=1):\n",
    "        self.__appliance = appliance\n",
    "        self.__algorithm = network_type\n",
    "        self.__network_type = network_type\n",
    "        self.__crop = crop\n",
    "        self.__batch_size = batch_size\n",
    "        self.__epochs = epochs\n",
    "        self.__patience = patience\n",
    "        self.__min_delta = min_delta\n",
    "        self.__verbose = verbose\n",
    "        self.__loss = \"mse\"\n",
    "        self.__metrics = [\"mse\", \"msle\", \"mae\"]\n",
    "        self.__learning_rate = 0.001\n",
    "        self.__beta_1=0.9\n",
    "        self.__beta_2=0.999\n",
    "        self.__save_model_dir = save_model_dir\n",
    "\n",
    "        self.__input_window_length = input_window_length\n",
    "        self.__window_size = 2+self.__input_window_length\n",
    "        self.__window_offset = int((0.5 * self.__window_size) - 1)\n",
    "        self.__max_chunk_size = 5 * 10 ** 2\n",
    "        self.__validation_frequency = validation_frequency\n",
    "        self.__ram_threshold=5*10**5\n",
    "        self.__skip_rows_train=10000000\n",
    "        self.__validation_steps=100\n",
    "        self.__skip_rows_val = 0\n",
    "\n",
    "        # Directories of the training and validation files. Always has the structure \n",
    "        # ./dataset_management/refit/{appliance_name}/{appliance_name}_training_.csv for training or \n",
    "        # ./dataset_management/refit/{appliance_name}/{appliance_name}_validation_.csv\n",
    "        self.__training_directory = training_directory\n",
    "        self.__validation_directory = validation_directory\n",
    "\n",
    "        self.__training_chunker = TrainSlidingWindowGenerator(file_name=self.__training_directory, \n",
    "                                        chunk_size=self.__max_chunk_size, \n",
    "                                        batch_size=self.__batch_size, \n",
    "                                        crop=self.__crop, shuffle=True,\n",
    "                                        skip_rows=self.__skip_rows_train, \n",
    "                                        offset=self.__window_offset, \n",
    "                                        ram_threshold=self.__ram_threshold)\n",
    "        self.__validation_chunker = TrainSlidingWindowGenerator(file_name=self.__validation_directory, \n",
    "                                            chunk_size=self.__max_chunk_size, \n",
    "                                            batch_size=self.__batch_size, \n",
    "                                            crop=self.__crop, \n",
    "                                            shuffle=True,\n",
    "                                            skip_rows=self.__skip_rows_val, \n",
    "                                            offset=self.__window_offset, \n",
    "                                            ram_threshold=self.__ram_threshold)\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        \"\"\" Trains an energy disaggregation model using a user-selected pruning algorithm (default is no pruning). \n",
    "        Plots and saves the resulting model. \"\"\"\n",
    "\n",
    "        # Calculate the optimum steps per epoch.\n",
    "        # self.__training_chunker.check_if_chunking()\n",
    "        #steps_per_training_epoch = np.round(int(self.__training_chunker.total_size / self.__batch_size), decimals=0)\n",
    "        steps_per_training_epoch = np.round(int(self.__training_chunker.total_num_samples / self.__batch_size), decimals=0)\n",
    "        \n",
    "        model = create_model(self.__input_window_length)\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.__learning_rate, beta_1=self.__beta_1, beta_2=self.__beta_2), loss=self.__loss, metrics=self.__metrics) \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=self.__min_delta, patience=self.__patience, verbose=self.__verbose, mode=\"auto\")\n",
    "\n",
    "        ## can use checkpoint ###############################################\n",
    "        # checkpoint_filepath = \"checkpoint/housedata/refit/\"+ self.__appliance + \"/\"\n",
    "        # model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #     filepath = checkpoint_filepath,\n",
    "        #     monitor='val_loss',\n",
    "        #     verbose=0,\n",
    "        #     save_best_only=True,\n",
    "        #     save_weights_only=False,\n",
    "        #     mode='auto',\n",
    "        #     save_freq='epoch')        \n",
    "        #callbacks=[early_stopping, model_checkpoint_callback]\n",
    "        ###################################################################\n",
    "\n",
    "        callbacks=[early_stopping]\n",
    "        \n",
    "        training_history = self.default_train(model, callbacks, steps_per_training_epoch)\n",
    "\n",
    "        training_history.history[\"val_loss\"] = np.repeat(training_history.history[\"val_loss\"], self.__validation_frequency)\n",
    "\n",
    "        model.summary()\n",
    "        save_model(model, self.__network_type, self.__algorithm, \n",
    "                   self.__appliance, self.__save_model_dir)\n",
    "\n",
    "        self.plot_training_results(training_history)\n",
    "\n",
    "    def default_train(self, model, callbacks, steps_per_training_epoch):\n",
    "\n",
    "        \"\"\" The default training method the neural network will use. No pruning occurs.\n",
    "\n",
    "        Parameters:\n",
    "        model (tensorflow.keras.Model): The seq2point model being trained.\n",
    "        early_stopping (tensorflow.keras.callbacks.EarlyStopping): An early stopping callback to \n",
    "        prevent overfitting.\n",
    "        steps_per_training_epoch (int): The number of training steps to occur per epoch.\n",
    "\n",
    "        Returns:\n",
    "        training_history (numpy.ndarray): The error metrics and loss values that were calculated \n",
    "        at the end of each training epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        # ########### this is retired ##############################\n",
    "        # training_history = model.fit_generator(self.__training_chunker.load_dataset(),\n",
    "        #     steps_per_epoch=steps_per_training_epoch,\n",
    "        #     epochs=1,\n",
    "        #     verbose=1,\n",
    "        #     validation_data = self.__validation_chunker.load_dataset(),\n",
    "        #     validation_steps=100,\n",
    "        #     validation_freq=self.__validation_frequency,\n",
    "        #     callbacks=[early_stopping])\n",
    "        ############################################################\n",
    "\n",
    "        training_history = model.fit(self.__training_chunker.load_dataset(),                            \n",
    "                      steps_per_epoch=int(steps_per_training_epoch),\n",
    "                      epochs=self.__epochs,\n",
    "                      verbose=self.__verbose,\n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=self.__validation_chunker.load_dataset(),\n",
    "                      validation_freq=self.__validation_frequency,\n",
    "                      validation_steps=self.__validation_steps)\n",
    "\n",
    "        return training_history\n",
    "\n",
    "    def plot_training_results(self, training_history):\n",
    "\n",
    "        \"\"\" Plots and saves a graph of training loss against epoch.\n",
    "\n",
    "        Parameters:\n",
    "        training_history (numpy.ndarray): A timeseries of loss against epoch count.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        plt.plot(training_history.history[\"loss\"], label=\"MSE (Training Loss)\")\n",
    "        plt.plot(training_history.history[\"val_loss\"], label=\"MSE (Validation Loss)\")\n",
    "        plt.title('Training History')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        #file_name = \"./\" + self.__appliance + \"/saved_models/\" + self.__appliance + \"_\" + self.__pruning_algorithm + \"_\" + self.__network_type + \"_training_results.png\"\n",
    "        #plt.savefig(fname=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559549f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np \n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Tester():\n",
    "\n",
    "    \"\"\" Used to test and evaluate a pre-trained seq2point model with or without pruning applied. \n",
    "    \n",
    "    Parameters:\n",
    "    __appliance (string): The target appliance.\n",
    "    __algorithm (string): The (pruning) algorithm the model was trained with.\n",
    "    __network_type (string): The architecture of the model.\n",
    "    __crop (int): The maximum number of rows of data to evaluate the model with.\n",
    "    __batch_size (int): The number of rows per testing batch.\n",
    "    __window_size (int): The size of eaech sliding window\n",
    "    __window_offset (int): The offset of the inferred value from the sliding window.\n",
    "    __test_directory (string): The directory of the test file for the model.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, appliance, algorithm, crop, batch_size, network_type,\n",
    "                 test_directory, saved_model_dir, log_file_dir,\n",
    "                 input_window_length):\n",
    "        self.__appliance = appliance\n",
    "        self.__algorithm = algorithm\n",
    "        self.__network_type = network_type\n",
    "\n",
    "        self.__crop = crop\n",
    "        self.__batch_size = batch_size\n",
    "        self._input_window_length = input_window_length\n",
    "        self.__window_size = self._input_window_length + 2\n",
    "        self.__window_offset = int(0.5 * self.__window_size - 1)\n",
    "        self.__number_of_windows = 100\n",
    "\n",
    "        self.__test_directory = test_directory\n",
    "        self.__saved_model_dir = saved_model_dir\n",
    "\n",
    "        self.__log_file = log_file_dir\n",
    "        logging.basicConfig(filename=self.__log_file,level=logging.INFO)\n",
    "\n",
    "    def test_model(self):\n",
    "\n",
    "        \"\"\" Tests a fully-trained model using a sliding window generator as an input. Measures inference time, gathers, and \n",
    "        plots evaluationg metrics. \"\"\"\n",
    "\n",
    "        test_input, test_target = self.load_dataset(self.__test_directory)\n",
    "        model = create_model(self._input_window_length)\n",
    "        model = load_model(model, self.__network_type, self.__algorithm, \n",
    "                           self.__appliance, self.__saved_model_dir)\n",
    "\n",
    "        test_generator = TestSlidingWindowGenerator(number_of_windows=self.__number_of_windows, inputs=test_input, targets=test_target, offset=self.__window_offset)\n",
    "\n",
    "        # Calculate the optimum steps per epoch.\n",
    "        steps_per_test_epoch = np.round(int(test_generator.total_size / self.__batch_size), decimals=0)\n",
    "\n",
    "        # Test the model.\n",
    "        start_time = time.time()\n",
    "        testing_history = model.predict(x=test_generator.load_dataset(), steps=steps_per_test_epoch, verbose=2)\n",
    "\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "\n",
    "        evaluation_metrics = model.evaluate(x=test_generator.load_dataset(), steps=steps_per_test_epoch)\n",
    "\n",
    "        self.log_results(model, test_time, evaluation_metrics)\n",
    "        self.plot_results(testing_history, test_input, test_target)\n",
    "\n",
    "\n",
    "    def load_dataset(self, directory):\n",
    "        \"\"\"Loads the testing dataset from the location specified by file_name.\n",
    "\n",
    "        Parameters:\n",
    "        directory (string): The location at which the dataset is stored, concatenated with the file name.\n",
    "\n",
    "        Returns:\n",
    "        test_input (numpy.array): The first n (crop) features of the test dataset.\n",
    "        test_target (numpy.array): The first n (crop) targets of the test dataset.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data_frame = pd.read_csv(directory, nrows=self.__crop, skiprows=0, header=0)\n",
    "        test_input = np.round(np.array(data_frame.iloc[:, 0], float), 6)\n",
    "        test_target = np.round(np.array(data_frame.iloc[self.__window_offset: -self.__window_offset, 1], float), 6)\n",
    "        \n",
    "        del data_frame\n",
    "        return test_input, test_target\n",
    "\n",
    "    def log_results(self, model, test_time, evaluation_metrics):\n",
    "\n",
    "        \"\"\"Logs the inference time, MAE and MSE of an evaluated model.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The evaluated model.\n",
    "        test_time (float): The time taken by the model to infer all required values.\n",
    "        evaluation metrics (list): The MSE, MAE, and various compression ratios of the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        inference_log = \"Inference Time: \" + str(test_time)\n",
    "        logging.info(inference_log)\n",
    "\n",
    "        metric_string = \"MSE: \", str(evaluation_metrics[0]), \" MAE: \", str(evaluation_metrics[3])\n",
    "        logging.info(metric_string)\n",
    "\n",
    "        self.count_pruned_weights(model)  \n",
    "\n",
    "    def count_pruned_weights(self, model):\n",
    "\n",
    "        \"\"\" Counts the total number of weights, pruned weights, and weights in convolutional \n",
    "        layers. Calculates the sparsity ratio of different layer types and logs these values.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The evaluated model.\n",
    "\n",
    "        \"\"\"\n",
    "        num_total_zeros = 0\n",
    "        num_dense_zeros = 0\n",
    "        num_dense_weights = 0\n",
    "        num_conv_zeros = 0\n",
    "        num_conv_weights = 0\n",
    "        for layer in model.layers:\n",
    "            if np.shape(layer.get_weights())[0] != 0:\n",
    "                layer_weights = layer.get_weights()[0].flatten()\n",
    "\n",
    "                if \"conv\" in layer.name:\n",
    "                    num_conv_weights += np.size(layer_weights)\n",
    "                    num_conv_zeros += np.count_nonzero(layer_weights==0)\n",
    "\n",
    "                    num_total_zeros += np.size(layer_weights)\n",
    "                else:\n",
    "                    num_dense_weights += np.size(layer_weights)\n",
    "                    num_dense_zeros += np.count_nonzero(layer_weights==0)\n",
    "\n",
    "        conv_zeros_string = \"CONV. ZEROS: \" + str(num_conv_zeros)\n",
    "        conv_weights_string = \"CONV. WEIGHTS: \" + str(num_conv_weights)\n",
    "        conv_sparsity_ratio = \"CONV. RATIO: \" + str(num_conv_zeros / num_conv_weights)\n",
    "\n",
    "        dense_weights_string = \"DENSE WEIGHTS: \" + str(num_dense_weights)\n",
    "        dense_zeros_string = \"DENSE ZEROS: \" + str(num_dense_zeros)\n",
    "        dense_sparsity_ratio = \"DENSE RATIO: \" + str(num_dense_zeros / num_dense_weights)\n",
    "\n",
    "        total_zeros_string = \"TOTAL ZEROS: \" + str(num_total_zeros)\n",
    "        total_weights_string = \"TOTAL WEIGHTS: \" + str(model.count_params())\n",
    "        total_sparsity_ratio = \"TOTAL RATIO: \" + str(num_total_zeros / model.count_params())\n",
    "\n",
    "        print(\"LOGGING PATH: \", self.__log_file)\n",
    "\n",
    "        logging.info(conv_zeros_string)\n",
    "        logging.info(conv_weights_string)\n",
    "        logging.info(conv_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(dense_zeros_string)\n",
    "        logging.info(dense_weights_string)\n",
    "        logging.info(dense_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(total_zeros_string)\n",
    "        logging.info(total_weights_string)\n",
    "        logging.info(total_sparsity_ratio)\n",
    "\n",
    "    def plot_results(self, testing_history, test_input, test_target):\n",
    "\n",
    "        \"\"\" Generates and saves a plot of the testing history of the model against the (actual) \n",
    "        aggregate energy values and the true appliance values.\n",
    "\n",
    "        Parameters:\n",
    "        testing_history (numpy.ndarray): The series of values inferred by the model.\n",
    "        test_input (numpy.ndarray): The aggregate energy data.\n",
    "        test_target (numpy.ndarray): The true energy values of the appliance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        testing_history = ((testing_history * params_appliance[self.__appliance][\"std\"]) + params_appliance[self.__appliance][\"mean\"])\n",
    "        test_target = ((test_target * params_appliance[self.__appliance][\"std\"]) + params_appliance[self.__appliance][\"mean\"])\n",
    "        test_agg = (test_input.flatten() * mains_data[\"std\"]) + mains_data[\"mean\"]\n",
    "        test_agg = test_agg[:testing_history.size]\n",
    "\n",
    "        # Can't have negative energy readings - set any results below 0 to 0.\n",
    "        test_target[test_target < 0] = 0\n",
    "        testing_history[testing_history < 0] = 0\n",
    "        test_input[test_input < 0] = 0\n",
    "\n",
    "        # Plot testing outcomes against ground truth.\n",
    "        plt.figure(1)\n",
    "        plt.plot(test_agg[self.__window_offset: -self.__window_offset], label=\"Aggregate\")\n",
    "        plt.plot(test_target[:test_agg.size - (2 * self.__window_offset)], label=\"Ground Truth\")\n",
    "        plt.plot(testing_history[:test_agg.size - (2 * self.__window_offset)], label=\"Predicted\")\n",
    "        plt.title(self.__appliance + \" \" + self.__network_type + \"(\" + self.__algorithm + \")\")\n",
    "        plt.ylabel(\"Power Value (Watts)\")\n",
    "        plt.xlabel(\"Testing Window\")\n",
    "        plt.legend()\n",
    "\n",
    "        #file_path = \"./\" + self.__appliance + \"/saved_models/\" + self.__appliance + \"_\" + self.__algorithm + \"_\" + self.__network_type + \"_test_figure.png\"\n",
    "        #plt.savefig(fname=file_path)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd317b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing training file...\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      7\u001b[0m     appliance\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mappliance_name,\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#verbose=1\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 111\u001b[0m, in \u001b[0;36mTrainer.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m## can use checkpoint ###############################################\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# checkpoint_filepath = \"checkpoint/housedata/refit/\"+ self.__appliance + \"/\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m#callbacks=[early_stopping, model_checkpoint_callback]\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m###################################################################\u001b[39;00m\n\u001b[0;32m    109\u001b[0m callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[1;32m--> 111\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_training_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m training_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(training_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__validation_frequency)\n\u001b[0;32m    115\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[6], line 147\u001b[0m, in \u001b[0;36mTrainer.default_train\u001b[1;34m(self, model, callbacks, steps_per_training_epoch)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" The default training method the neural network will use. No pruning occurs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# ########### this is retired ##############################\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# training_history = model.fit_generator(self.__training_chunker.load_dataset(),\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#     steps_per_epoch=steps_per_training_epoch,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#     callbacks=[early_stopping])\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m############################################################\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__training_chunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m              \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps_per_training_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_chunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_history\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[3], line 88\u001b[0m, in \u001b[0;36mTrainSlidingWindowGenerator.load_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields pairs of features and targets that will be used directly by a neural network for training.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__total_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_if_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# If the data can be loaded in one go, don't skip any rows.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__total_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ram_threshold):\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Returns an array of the content from the CSV file.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mTrainSlidingWindowGenerator.check_if_chunking\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Loads the file and counts the number of rows it contains.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImporting training file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__crop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__skip_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCounting number of rows...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunks)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "# Allows a model to be trained from the terminal.\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    appliance= args.appliance_name,\n",
    "    batch_size=args.batch_size,\n",
    "    crop=args.crop,\n",
    "    network_type=args.network_type,\n",
    "    training_directory=args.training_directory,\n",
    "    validation_directory=args.validation_directory,\n",
    "    save_model_dir=save_model_dir,\n",
    "    epochs=args.epochs,\n",
    "    input_window_length=args.input_window_length,\n",
    "    validation_frequency=args.validation_frequency\n",
    "    #patience=3,\n",
    "    #min_delta= 1e-6,\n",
    "    #verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
